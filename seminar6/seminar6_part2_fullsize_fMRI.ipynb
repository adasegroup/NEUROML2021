{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "seminar6_part2_fullsize_fMRI.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kfEsIDcSx0-H"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZIZlv5Hs2e-"
      },
      "source": [
        "<a href=\"https://drive.google.com/file/d/1ZE0L54UDA2p-vRZs79BvLDjEp6bMY9Ox/view?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# **Seminar 6: Deep Learning for fMRI**\n",
        "\n",
        "## **Classification of full-size fMRI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMj175Fw7_eP"
      },
      "source": [
        "#### Introduction\n",
        "In this notebook we will work with full-size fMRI data which represents 4D tensor or a 3D video of the brain functional activity.\n",
        "\n",
        "**We will train a network for detection of Autistm Spectrum Disorder (ASD) based on full-size fMRI series.** \n",
        "\n",
        "**Also, we will apply a conventional domain adaptation approach to reduce the part of the site-related variability in the data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPx5Ba3ks2fA"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import nibabel as nib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MjR5PKZJpsm"
      },
      "source": [
        "# check if gpu is available\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpRdaVYgJtfI"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "if use_cuda:\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    print(\"Not using GPU\")\n",
        "device = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lakVWOUC9Ao0"
      },
      "source": [
        "Mounting Google Drive to Collab Notebook. You should go with the link and enter your personal authorization code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZkNPQnzvCHM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A_hJE5g9Dxv"
      },
      "source": [
        "Get the data. Add a shortcut to your Google Drive.\n",
        "\n",
        "Shared link: https://drive.google.com/drive/folders/1_63qnHOCUEzOUmUWhcmTXulmQMmJglwT?usp=sharing\n",
        "\n",
        "(You will need the same data directory, as in the first part of the seminar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHZJKicK9ngX"
      },
      "source": [
        "We will use full-size fMRI series from the same data collection, that we analysed in the first part. However,  here we will only consider the data from **2** acquistion sites - **USM** and **UCLA** with around **70** participants from each. \n",
        "\n",
        "\n",
        "<!-- The diagnosis is labelled in **\"DX_GROUP\"** column and is either Autism spectrum disorder or Healthy control. -->\n",
        "\n",
        "<!-- You may also see that data collection is composed of smaller datasets provided from several different medical centers and research institutes (see the **\"SOURCE\"** column). -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZfznwFb9fPd"
      },
      "source": [
        "folder_path = '/content/drive/My Drive/NeuroML/func_ABIDE/abide_fmri/'\n",
        "targets_path = '/content/drive/My Drive/NeuroML/func_ABIDE/ABIDE1CPAC_targets.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4A2xz0u0__H"
      },
      "source": [
        "### Dataloader to load full-size fMRI data. \n",
        "\n",
        "Below you can see a Dataset class for loading full-size fMRI. \n",
        "\n",
        "What is implemented here:\n",
        "\n",
        "- Collecting all the fMRI files from a given folder;\n",
        "- Loading fMRI 4D time series from a `.nii` of `.npy` file;\n",
        "- **Preloading** all the images into RAM or **loading them online** (`load_online` argument);\n",
        "- **Cropping a brain image** to the given size, if needed (`coord_min` and `img_shape` arguments);\n",
        "- **Taking a subsequence** of a given size starting from a given time step, or sampling at a random time step, if not given (`start_pos`, `seq_len` arguments)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4Dbi6ins2fj"
      },
      "source": [
        "def load_nii_to_array(nii_path):\n",
        "    return np.asanyarray(nib.load(nii_path).dataobj)\n",
        "\n",
        "class fMRIDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        folder_path: path to data folder\n",
        "        labels_path: path to file with targets and additional information\n",
        "        target: column of targets df with target to predict. If None, loads images only\n",
        "        encode_target: if True, encode target with LabelEncoder\n",
        "        mri_file_suffix (str): consider only files with given keyword in filename\n",
        "        load_online (bool): if True, load mri images online. Else, preload everything during initialization\n",
        "        transform (torchvision.transforms): if given, apply transformation to the item. If None, return item without modifications\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, folder_path, labels_path, target=None, encode_target=False, domain_target=None,\n",
        "                 mri_file_suffix=\"\", load_online=False, transform=None,\n",
        "                 source_col=\"SOURCE\", use_sources=[],\n",
        "                 coord_min=(6, 5, 6), img_shape=(48, 64, 48), \n",
        "                 start_pos=None, seq_len=None):\n",
        "        self.mri_paths = {\n",
        "            \"participant_id\" : [],\n",
        "            \"path\" : [],\n",
        "        }\n",
        "        \n",
        "        self.folder_path = folder_path\n",
        "        self.labels = pd.read_csv(labels_path)\n",
        "        self.target, self.domain_target = None, None\n",
        "        self.load_online = load_online\n",
        "        \n",
        "        self.mri_file_suffix = mri_file_suffix\n",
        "        self.source_col = source_col\n",
        "        self.use_sources = use_sources\n",
        "\n",
        "        self.coord_min = coord_min\n",
        "        self.img_shape = img_shape\n",
        "        self.start_pos = start_pos\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transform\n",
        "\n",
        "        def get_participant_id(participant_file):\n",
        "            return \"sub-\" + participant_file.split(\"_\")[-4]\n",
        "        \n",
        "        participant_files = os.listdir(self.folder_path)\n",
        "        for participant_file in tqdm(participant_files):\n",
        "            if (self.mri_file_suffix in participant_file):\n",
        "                # is participant file\n",
        "                participant_path = os.path.join(self.folder_path, participant_file)\n",
        "                participant_id = get_participant_id(participant_file)\n",
        "                self.mri_paths[\"path\"].append(participant_path)\n",
        "                self.mri_paths[\"participant_id\"].append(participant_id)\n",
        "                            \n",
        "        self.mri_paths = pd.DataFrame(self.mri_paths)\n",
        "        self.labels = self.labels.merge(self.mri_paths, on=\"participant_id\")\n",
        "        self.mri_files = self.labels[\"path\"].tolist()\n",
        "        print(f\"{len(self.mri_files)} image files found.\")\n",
        "        \n",
        "        if not self.load_online:\n",
        "            self.mri_files = [self.get_image(index, self.start_pos, self.seq_len) for index in tqdm(range(len(self.mri_files)))]\n",
        "\n",
        "        # update self.img_shape (and other params ?)\n",
        "        self.output_img_shape = self[0].shape[1:4]\n",
        "        self.target, self.domain_target = self.set_target(target, encode_target, domain_target)\n",
        "        \n",
        "    \n",
        "    def set_target(self, target=None, encode_target=False, domain_target=None):\n",
        "        self.target, self.domain_target = None, None\n",
        "        if target is not None:\n",
        "            self.target = self.labels[target].copy()\n",
        "            if (self.source_col is not None) and self.use_sources:\n",
        "                # preserve only targets for objects from sources of interest\n",
        "                null_idx = ~self.labels[self.source_col].isin(self.use_sources)\n",
        "                self.target[null_idx] = np.nan\n",
        "            if encode_target:\n",
        "                enc = LabelEncoder()\n",
        "                idx = self.target.notnull()\n",
        "                self.target[idx] = enc.fit_transform(self.target[idx])\n",
        "            if domain_target is not None:\n",
        "                self.domain_target = self.labels[domain_target].copy()\n",
        "                if (self.source_col is not None) and self.use_sources:\n",
        "                    # preserve only targets for objects from sources of interest\n",
        "                    null_idx = ~self.labels[self.source_col].isin(self.use_sources)\n",
        "                    self.domain_target[null_idx] = np.nan\n",
        "                self.domain_enc = LabelEncoder()\n",
        "                idx = self.domain_target.notnull()\n",
        "                self.domain_target[idx] = self.domain_enc.fit_transform(self.domain_target[idx])\n",
        "        return self.target, self.domain_target\n",
        "\n",
        "            \n",
        "    def reshape_image(self, mri_img, coord_min, img_shape):\n",
        "        seq_len = mri_img.shape[-1]\n",
        "        return mri_img[coord_min[0]:coord_min[0] + img_shape[0],\n",
        "                        coord_min[1]:coord_min[1] + img_shape[1],\n",
        "                        coord_min[2]:coord_min[2] + img_shape[2], :].reshape((1,) + img_shape + (seq_len,))\n",
        "        \n",
        "    def get_image(self, index, start_pos=None, seq_len=None):\n",
        "        \n",
        "        def load_mri(mri_file):\n",
        "            if \"nii\" in mri_file:\n",
        "                img = load_nii_to_array(mri_file)\n",
        "            else:\n",
        "                img = np.load(mri_file)\n",
        "            return img\n",
        "        \n",
        "        mri_file = self.mri_files[index]\n",
        "        img = load_mri(mri_file)        \n",
        "        img = self.reshape_image(img, self.coord_min, self.img_shape)\n",
        "        \n",
        "        if seq_len is None:\n",
        "            seq_len = img.shape[-1]\n",
        "        if start_pos is None:\n",
        "            start_pos = np.random.choice(np.arange(img.shape[-1] - seq_len + 1))\n",
        "        if seq_len == 1:\n",
        "            img = img[:, :, :, :, start_pos]\n",
        "        else:\n",
        "            img = img[:, :, :, :, start_pos:start_pos + seq_len]\n",
        "            img = img.transpose((4, 0, 1, 2, 3))\n",
        "        return img\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        if (self.source_col is not None) and self.use_sources:\n",
        "            s = self.labels[self.source_col][index]\n",
        "            if s not in self.use_sources:\n",
        "                return None\n",
        "\n",
        "        img = self.get_image(index, self.start_pos, self.seq_len) if self.load_online else self.mri_files[index]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target is None:\n",
        "            item = img\n",
        "        else:\n",
        "            item = [img, self.target[index]]\n",
        "            if self.domain_target is not None:\n",
        "                item += [self.domain_target[index]]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mri_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouARZ4ir2F1L"
      },
      "source": [
        "\n",
        "<!-- 2. Individually normalizing each image to have given `min` and `max` values (here **0** and **1**). -->\n",
        "\n",
        "\n",
        "Transforms are applied to each image returned by `fMRIDataset` object. For now, we will only need to transform the fMRI image from `np.array` to `torch.tensor`. \n",
        "However, you can implement additional transformation to add various types of augmentations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0b51CscR05M"
      },
      "source": [
        "# transforms\n",
        "import warnings\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, img):\n",
        "        return torch.FloatTensor(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t57ZzM1_nZ5b"
      },
      "source": [
        "### Load data\n",
        "\n",
        "Create a dataset that loads data from `func_ABIDE/abide_fmri`.\n",
        "\n",
        "In general, fMRI sequences are too large, so we cannot process the whole sequence with neural network on GPU. A common practice is to **select random subsequences at the training** stage, and at the testing,  to split the sequence into several parts and **average the predictions** over them.\n",
        "\n",
        "However, now, for faster training, we will use a prepared dataset of short subsequences from **0**th to **16**th time step of each series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkLWqRDqLleU"
      },
      "source": [
        "# create the dataset and load fMRI sequences\n",
        "\n",
        "folder_path = '/content/drive/My Drive/NeuroML/func_ABIDE/abide_fmri'\n",
        "targets_path = '/content/drive/My Drive/NeuroML/func_ABIDE/ABIDE1CPAC_targets.csv'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = fMRIDataset(\n",
        "    folder_path, \n",
        "    labels_path=targets_path,\n",
        "    target=None,\n",
        "    mri_file_suffix=\"npy\",\n",
        "    load_online=False,\n",
        "    transform=transform,\n",
        "    source_col=\"SOURCE\",\n",
        "    use_sources=[\"USM\", \"UCLA\"],\n",
        "    start_pos=0,\n",
        "    seq_len=16,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VMvJ8z_FM2m"
      },
      "source": [
        "Take a look at the data. We will plot a brain image for a single time step. High-intensity values ​​correspond to strong brain activity in a given region."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0W7bkWLs2fn"
      },
      "source": [
        "img = dataset[0]\n",
        "print(img.shape)\n",
        "\n",
        "img_t = img[0, 0,]\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(131)\n",
        "plt.imshow(img_t[img_t.shape[0] // 2, :, :])\n",
        "plt.subplot(132)\n",
        "plt.imshow(img_t[:, img_t.shape[1] // 2, :])\n",
        "plt.subplot(133)\n",
        "plt.imshow(img_t[:, :, img_t.shape[2] // 2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4FZzU2rs2fi"
      },
      "source": [
        "## **ConvTempNet for 4D full-size fMRI**\n",
        "\n",
        "Now, we will train a ConvTempNet model to analyze this data and distinguish **patients with autism spectrum disorder** from the **healthy control group**. \n",
        "\n",
        "First, remind that ConvTempNet arhitecture consisits of two parts - convolutional and recurrent. It allows us to first extract vector of spatial features from each 3D brain image with **3D CNN**, and then process this sequence of vectors with **RNN** (or another suitable model, for example, temporal convoltions) to capture temporal patterns. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnpcbAJmtX0p"
      },
      "source": [
        "### Convolutional blocks\n",
        "\n",
        "**ConvEncoder** is an intermediate model, that extracts vector of spatial features from each 3D image in fMRI sequence and returns sequence of feature vectores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "ifo-_I4Fs2fw"
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class CNN3d(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_shape=(64, 64, 64), \n",
        "                 n_outputs=None, \n",
        "                 conv_model=[32, 64, 128, 256], \n",
        "                 n_flatten_units=None):\n",
        "        super(self.__class__, self).__init__()\n",
        "        input_shape = np.array(input_shape)\n",
        "        self.n_layers = len(conv_model)\n",
        "        \n",
        "        self.model = []\n",
        "        C_in = 1\n",
        "        for C_out in conv_model:\n",
        "            self.model += [\n",
        "                nn.Conv3d(C_in, C_out, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm3d(C_out),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            C_in = C_out\n",
        "            \n",
        "        self.model += [Flatten()]\n",
        "        print(\"n activations:\", C_out,\n",
        "              \"of size:\", list(input_shape // (2 ** self.n_layers)))\n",
        "        self.n_flatten_units = int(np.prod(input_shape // (2 ** self.n_layers)) * C_out)\n",
        "        print(\"n flatten units:\", self.n_flatten_units)\n",
        "        if n_outputs is not None:\n",
        "            self.model += [\n",
        "                nn.Linear(self.n_flatten_units, n_outputs)\n",
        "            ]\n",
        "        self.model = nn.Sequential(*self.model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "\n",
        "class ResBlock3d(nn.Module):\n",
        "    def __init__(self, C):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.C = C\n",
        "        self.block = [\n",
        "                nn.BatchNorm3d(C),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv3d(C, C, kernel_size=3, padding=1),\n",
        "            ]\n",
        "        self.block = nn.Sequential(*self.block)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "    \n",
        "\n",
        "class ResNet3d(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_shape=(64, 64, 64), \n",
        "                 n_outputs=None, \n",
        "                 conv_model=[32, 64, 128, 256], \n",
        "                 n_flatten_units=None):\n",
        "        super(self.__class__, self).__init__()\n",
        "        input_shape = np.array(input_shape)\n",
        "        self.n_layers = len(conv_model)\n",
        "        \n",
        "        self.model = []\n",
        "        C_in = 1\n",
        "        for C_out in conv_model:\n",
        "            # here instead of 3x3-BN-ReLU\n",
        "            # we use 1x1-ResBlock3d-1x1 (1x1-BN-ReLU-3x3-1x1)\n",
        "            self.model += [\n",
        "                nn.Conv3d(C_in, C_out, 1, 1, 0, bias=False), # 1x1, add channels\n",
        "                ResBlock3d(C_out),\n",
        "                nn.Conv3d(C_out, C_out, 1, 2, 0, bias=False), # 1x1 reduce spatial dim\n",
        "            ]\n",
        "            C_in = C_out\n",
        "            \n",
        "        self.model += [Flatten()]\n",
        "        print(\"n activations:\", C_out,\n",
        "              \"of size:\", list(input_shape // (2 ** self.n_layers)))\n",
        "        self.n_flatten_units = int(np.prod(input_shape // (2 ** self.n_layers)) * C_out)\n",
        "        print(\"n flatten units:\", self.n_flatten_units)\n",
        "        if n_outputs is not None:\n",
        "            self.model += [\n",
        "                nn.Linear(self.n_flatten_units, n_outputs)\n",
        "            ]\n",
        "        self.model = nn.Sequential(*self.model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class ConvEncoder(nn.Module):\n",
        "    def __init__(self, input_shape=(64, 64, 64),\n",
        "                 conv_model=[32, 64, 128, 256], conv_model_type=\"CNN\",\n",
        "                 n_flatten_units=None):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.conv_model_type = conv_model_type\n",
        "        if conv_model_type == \"CNN\":\n",
        "            self.conv_model = CNN3d(\n",
        "                input_shape, None, \n",
        "                conv_model, n_flatten_units, \n",
        "            )\n",
        "        elif conv_model_type == \"ResNet\":\n",
        "            self.conv_model = ResNet3d(\n",
        "                input_shape, None, \n",
        "                conv_model, n_flatten_units, \n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        n_objects, seq_length = x.size()[0:2]\n",
        "        x = x.reshape([n_objects * seq_length] + list(x.size()[2:]))\n",
        "        x = self.conv_model(x)\n",
        "        x = x.reshape([n_objects, seq_length, -1])\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usWsKTWQtdjd"
      },
      "source": [
        "###  ClfGRU model\n",
        "\n",
        "**ClfGRU** is a recurrent model with GRU units, that takes sequence of feature vectors (in our case, extracted by ConvEncoder) and predicts the target variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N84tpN78s2f0"
      },
      "source": [
        "class ClfGRU(nn.Module):\n",
        "    def __init__(self, n_latent_units, seq_length, n_outputs, \n",
        "                 hidden_size=128, n_layers=1,\n",
        "                 n_fc_units=128, use_states=\"last\", dropout=0):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.n_latent_units = n_latent_units\n",
        "        self.seq_length = seq_length\n",
        "        self.n_outputs = n_outputs\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(\n",
        "            n_latent_units, \n",
        "            hidden_size, n_layers, \n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        self.use_states = use_states\n",
        "        if use_states == \"last\":\n",
        "            self.gru_out_size = hidden_size\n",
        "        elif use_states == \"mean\":\n",
        "            self.gru_out_size = hidden_size\n",
        "        elif use_states == \"all\":\n",
        "            self.gru_out_size = hidden_size * seq_length\n",
        "            \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(self.gru_out_size, n_fc_units)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(n_fc_units, n_outputs)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        \n",
        "        if self.use_states == \"last\":\n",
        "            out = out[:, -1, :]\n",
        "        elif self.use_states == \"mean\":\n",
        "            out = out.mean(dim=1)\n",
        "        elif self.use_states == \"all\":\n",
        "            out = out.reshape(n_objects, self.hidden_size * seq_length)\n",
        "        \n",
        "        out = self.dropout(out)\n",
        "        out = self.fc1(out)  \n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP-GKJd6uSPh"
      },
      "source": [
        "### Train and utils functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LODB7clGcU9"
      },
      "source": [
        "def compute_probs(outputs):\n",
        "    \"\"\"\n",
        "    Computes class probabilities from logits predicted by the model.\n",
        "    If classification problem is binary, outputs probabilities of the 1st class.\n",
        "    Else, outputs array of probabilities of each class. \n",
        "    \"\"\"\n",
        "    if outputs.size(1) <= 2:\n",
        "        probs = F.softmax(outputs, dim=-1)[:, 1]\n",
        "    else:\n",
        "        probs = F.softmax(outputs, dim=-1)\n",
        "    return probs\n",
        "\n",
        "\n",
        "def train(train_loader, val_loader, args,\n",
        "          C, T, C_opt, T_opt,\n",
        "          crit=nn.CrossEntropyLoss(),\n",
        "          metric=roc_auc_score):\n",
        "    \n",
        "    def plot_results(epoch):\n",
        "        clear_output(True)\n",
        "        print(\"EPOCH {}\".format(epoch))\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(121)\n",
        "        plt.plot(train_stats[\"mean_train_loss\"], label=\"train_loss\")\n",
        "        plt.plot(train_stats[\"mean_val_loss\"], label=\"val_loss\")\n",
        "        plt.title(\"losses\")\n",
        "        plt.legend()\n",
        "        plt.subplot(122)\n",
        "        plt.plot(train_stats[\"mean_train_metric\"], label=\"train_metric\")\n",
        "        plt.plot(train_stats[\"mean_val_metric\"], label=\"val_metric\")\n",
        "        plt.gca().set_ylim([0, 1])\n",
        "        plt.title(\"metrics\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_stats[\"mean_train_loss\"][-1]))\n",
        "        print(\"  validation loss: \\t\\t\\t{:.6f}\".format(train_stats[\"mean_val_loss\"][-1]))\n",
        "        print()\n",
        "        print(\"  training AUC: \\t\\t\\t{:.2f}\".format(train_stats[\"mean_train_metric\"][-1]))\n",
        "        print(\"  validation AUC: \\t\\t\\t{:.2f}\".format(train_stats[\"mean_val_metric\"][-1]))\n",
        "    \n",
        "    train_stats = {\n",
        "        \"mean_train_loss\" : [],\n",
        "        \"mean_val_loss\" : [],\n",
        "        \"mean_train_metric\" : [],\n",
        "        \"mean_val_metric\" : [],\n",
        "    }\n",
        "    \n",
        "    for epoch in range(args[\"n_epochs\"]):\n",
        "        print(\"TRAIN EPOCH {}...\".format(epoch))\n",
        "        train_loss, train_preds, train_targets = [], [], []\n",
        "        val_loss, val_preds, val_targets = [], [], []\n",
        "        \n",
        "        # train epoch\n",
        "        C.train(True), T.train(True)\n",
        "        for inputs_batch, targets_batch in tqdm(train_loader):\n",
        "            inputs_batch, targets_batch = inputs_batch.float().to(device), targets_batch.long().to(device)\n",
        "\n",
        "            latents_batch = C(inputs_batch)\n",
        "            logits_batch = T(latents_batch)\n",
        "            loss = crit(logits_batch, targets_batch)\n",
        "            loss.backward()\n",
        "            T_opt.step(), C_opt.step()\n",
        "            T_opt.zero_grad(), C_opt.zero_grad()\n",
        "            \n",
        "            train_loss.append(loss.item())\n",
        "            preds = compute_probs(logits_batch)\n",
        "            train_preds.extend(list(preds.cpu().data.numpy()))\n",
        "            train_targets.extend(list(targets_batch.cpu().data.numpy()))\n",
        "        train_loss = np.mean(train_loss)\n",
        "        train_metric = metric(train_targets, train_preds)\n",
        "        \n",
        "        # validate\n",
        "        C.train(False), T.train(False)\n",
        "        for inputs_batch, targets_batch in tqdm(val_loader):\n",
        "            inputs_batch, targets_batch = inputs_batch.float().to(device), targets_batch.long().to(device)\n",
        "            \n",
        "            latents_batch = C(inputs_batch)\n",
        "            logits_batch = T(latents_batch)\n",
        "            loss = crit(logits_batch, targets_batch)\n",
        "            val_loss.append(loss.item())\n",
        "            preds = compute_probs(logits_batch)\n",
        "            val_preds.extend(list(preds.cpu().data.numpy()))\n",
        "            val_targets.extend(list(targets_batch.cpu().data.numpy())) \n",
        "        val_metric = metric(val_targets, val_preds)\n",
        "\n",
        "        # save stats\n",
        "        train_stats[\"mean_train_loss\"].append(np.mean(train_loss))\n",
        "        train_stats[\"mean_val_loss\"].append(np.mean(val_loss))\n",
        "        train_stats[\"mean_train_metric\"].append(train_metric)\n",
        "        train_stats[\"mean_val_metric\"].append(val_metric)\n",
        "        \n",
        "        plot_results(epoch)        \n",
        "    return train_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6eLXsBOuoqo"
      },
      "source": [
        "### Training\n",
        "\n",
        "We will train the model on **3** data samples: from this **USM** only, from **UCLA** only, and from both **USM+UCLA**. \n",
        "\n",
        "For each sample, split data into training and validation parts, and create **train and val dataloaders**. \n",
        "\n",
        "Then, create **ConvEncoder** and **ClfGRU** models with parameters of your choice (don't make it too complex), optimizer and scheduler (if needed). Train the model to detect patients with ASD from healthy control and measure its **ROC AUC** on the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1fG5b_E1vLp"
      },
      "source": [
        "dataset.use_sources = [\"USM\"]\n",
        "\n",
        "# dataset have target classes 1 and 2\n",
        "# encode target with LabelEncoder to use classes 0 and 1 instead\n",
        "dataset.set_target(\"DX_GROUP\", encode_target=True)\n",
        "\n",
        "notnull_idx = dataset.target[dataset.target.notnull()].index\n",
        "train_idx, val_idx = train_test_split(notnull_idx, stratify=dataset.target[notnull_idx],\n",
        "                                      test_size=0.2, random_state=42)\n",
        "batch_size = 16\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    data.Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    data.Subset(dataset, val_idx), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "C_model = ConvEncoder(input_shape=(48, 64, 48), \n",
        "                      conv_model=[32, 64, 128, 256], \n",
        "                      conv_model_type=\"CNN\",\n",
        "                      n_flatten_units=None).to(device)\n",
        "T_model = ClfGRU(n_latent_units=C_model.conv_model.n_flatten_units, \n",
        "                 seq_length=16, \n",
        "                 n_outputs=2,\n",
        "                 hidden_size=64,\n",
        "                 n_layers=1,\n",
        "                 use_states=\"mean\",\n",
        "                 n_fc_units=128,\n",
        "                 dropout=0.2).to(device)\n",
        "lr = 1e-4\n",
        "wd = 0           \n",
        "C_opt = torch.optim.Adam(C_model.parameters(), lr=lr, weight_decay=wd)\n",
        "T_opt = torch.optim.Adam(T_model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "args = {\"n_epochs\" : 25}\n",
        "train_stats = train(train_loader, val_loader, args,\n",
        "                    C_model, T_model, C_opt, T_opt,\n",
        "                    crit=nn.CrossEntropyLoss(), metric=roc_auc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ViGw0nFN4x"
      },
      "source": [
        "# train on the data from UCLA only\n",
        "dataset.use_sources = [\"UCLA\"]\n",
        "\n",
        "# dataset have target classes 1 and 2\n",
        "# encode target with LabelEncoder to use classes 0 and 1 instead\n",
        "dataset.set_target(\"DX_GROUP\", encode_target=True)\n",
        "\n",
        "notnull_idx = dataset.target[dataset.target.notnull()].index\n",
        "train_idx, val_idx = train_test_split(notnull_idx, stratify=dataset.target[notnull_idx],\n",
        "                                      test_size=0.2, random_state=42)\n",
        "batch_size = 16\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    data.Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    data.Subset(dataset, val_idx), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "C_model = ConvEncoder(input_shape=(48, 64, 48), \n",
        "                      conv_model=[32, 64, 128, 256], \n",
        "                      conv_model_type=\"CNN\",\n",
        "                      n_flatten_units=None).to(device)\n",
        "T_model = ClfGRU(n_latent_units=C_model.conv_model.n_flatten_units, \n",
        "                 seq_length=16, \n",
        "                 n_outputs=2,\n",
        "                 hidden_size=64,\n",
        "                 n_layers=1,\n",
        "                 use_states=\"mean\",\n",
        "                 n_fc_units=128,\n",
        "                 dropout=0.2).to(device)\n",
        "lr = 1e-4\n",
        "wd = 0           \n",
        "C_opt = torch.optim.Adam(C_model.parameters(), lr=lr, weight_decay=wd)\n",
        "T_opt = torch.optim.Adam(T_model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "args = {\"n_epochs\" : 25}\n",
        "train_stats = train(train_loader, val_loader, args,\n",
        "                    C_model, T_model, C_opt, T_opt,\n",
        "                    crit=nn.CrossEntropyLoss(), metric=roc_auc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFCvl0DWFPYa"
      },
      "source": [
        "# train on the data from USM+UCLA\n",
        "dataset.use_sources = [\"USM\", \"UCLA\"]\n",
        "\n",
        "# dataset have target classes 1 and 2\n",
        "# encode target with LabelEncoder to use classes 0 and 1 instead\n",
        "dataset.set_target(\"DX_GROUP\", encode_target=True)\n",
        "\n",
        "notnull_idx = dataset.target[dataset.target.notnull()].index\n",
        "train_idx, val_idx = train_test_split(notnull_idx, stratify=dataset.target[notnull_idx],\n",
        "                                      test_size=0.2, random_state=42)\n",
        "batch_size = 16\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    data.Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    data.Subset(dataset, val_idx), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "C_model = ConvEncoder(input_shape=(48, 64, 48), \n",
        "                      conv_model=[32, 64, 128, 256], \n",
        "                      conv_model_type=\"CNN\",\n",
        "                      n_flatten_units=None).to(device)\n",
        "T_model = ClfGRU(n_latent_units=C_model.conv_model.n_flatten_units, \n",
        "                 seq_length=16, \n",
        "                 n_outputs=2,\n",
        "                 hidden_size=64,\n",
        "                 n_layers=1,\n",
        "                 use_states=\"mean\",\n",
        "                 n_fc_units=128,\n",
        "                 dropout=0.2).to(device)\n",
        "lr = 1e-4\n",
        "wd = 0           \n",
        "C_opt = torch.optim.Adam(C_model.parameters(), lr=lr, weight_decay=wd)\n",
        "T_opt = torch.optim.Adam(T_model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "args = {\"n_epochs\" : 25}\n",
        "train_stats = train(train_loader, val_loader, args,\n",
        "                    C_model, T_model, C_opt, T_opt,\n",
        "                    crit=nn.CrossEntropyLoss(), metric=roc_auc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmQcyjU9W5JX"
      },
      "source": [
        "### Latent reprsentations\n",
        "\n",
        "Now, let's take a look at the latent features learnt by convolutional model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20A3fhq0UuFk"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA33tKNRWdxy"
      },
      "source": [
        "latents = []\n",
        "dataset.set_target()\n",
        "C_model.train(False)\n",
        "for i in tqdm(notnull_idx):\n",
        "    x = dataset[i][[0]].unsqueeze(dim=0).to(device)\n",
        "    z = C_model(x)[0].cpu().detach().numpy()\n",
        "    latents.append(z)\n",
        "latents = np.concatenate(latents, axis=0)\n",
        "\n",
        "pca = PCA(n_components=50)\n",
        "tsne = TSNE(n_components=2)\n",
        "latents_2d = tsne.fit_transform(pca.fit_transform(latents))\n",
        "\n",
        "source_colors = {\"USM\" : \"tab:blue\",\n",
        "          \"UCLA\" : \"tab:orange\",}\n",
        "sources = np.array(dataset.labels.loc[notnull_idx, \"SOURCE\"].tolist())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for s in source_colors:\n",
        "    s_idx = (sources == s)\n",
        "    plt.scatter(latents_2d[s_idx, 0], latents_2d[s_idx, 1], \n",
        "                s=30, alpha=0.75, c=source_colors[s], label=s)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2LplZQMs2f7"
      },
      "source": [
        "## 3. Domain adaptation\n",
        "\n",
        "Let's try to make the model encode **less site-dependent** information.\n",
        "\n",
        "We will employ one of the most widespread domain adaptation approaches proposed by Ganin et al. [https://arxiv.org/pdf/1505.07818.pdf]. The model here is composed of 3 parts: \n",
        "- a **feature extractor**, that transforms an input (in our case, an fMRI image) into a vector of high-level features,\n",
        "- a **classifier (predictor)** - the main model, that takes extracted features and predicts the target;\n",
        "- a **discriminator**, that takes extracted features and predicts the **domain** on the original sample. \n",
        "\n",
        "During optimization, the gradients from discriminator to feature extractor are **reversed**. So, the feature extractor is forced to learn the features, that are **as less informative as possible** for the discriminator, that is, contain **less information on the domain**. \n",
        "\n",
        "\n",
        "The whole architecture is presented below:\n",
        "<img src=\"https://miro.medium.com/max/1082/1*9aVg6JGcZFSKHUn_U4RrUQ.png\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvCmT0S6xP_M"
      },
      "source": [
        "### Discriminator for DANN\n",
        "\n",
        "As a discriminator we will use a simple model that takes feature vector of each time step, applies fully-connected network to it to predict probability of each class (domain) and then averages the predcitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsUh9r4_9SFY"
      },
      "source": [
        "# discriminator to predict domain (site) of a sample\n",
        "class DiscModel(nn.Module):\n",
        "    def __init__(self, n_latent_units, \n",
        "                 n_domains=2,\n",
        "                 fc_model=[1024, 256, 64], \n",
        "                 batchnorm=True,\n",
        "                 dropout=0):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.n_latent_units = n_latent_units\n",
        "        self.n_outputs = n_domains\n",
        "\n",
        "        self.model = []\n",
        "        n_in = n_latent_units\n",
        "        for n_out in fc_model:\n",
        "            self.model += [nn.Dropout(dropout),\n",
        "                           nn.Linear(n_in, n_out)]\n",
        "            if batchnorm:\n",
        "                self.model += [nn.BatchNorm1d(n_out)]\n",
        "            self.model += [\n",
        "                nn.ReLU(True),\n",
        "            ]\n",
        "            n_in = n_out\n",
        "        self.model += [nn.Dropout(dropout),\n",
        "                       nn.Linear(n_in, self.n_outputs)]\n",
        "        self.model = nn.Sequential(*self.model)\n",
        "\n",
        "                    \n",
        "    def forward(self, x): \n",
        "        # reshape to (n_obj * n_time_steps, latent_size)\n",
        "        n_objects, seq_length = x.size()[0:2]\n",
        "        x = x.reshape([n_objects * seq_length] + list(x.size()[2:]))\n",
        "        # apply fc model\n",
        "        x = self.model(x)\n",
        "        # reshape to (n_obj, n_time_steps, logit), mean over time_steps\n",
        "        x = x.reshape([n_objects, seq_length, -1])\n",
        "        x = x.mean(dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfEsIDcSx0-H"
      },
      "source": [
        "### DANN train functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRTPY-No2xoB"
      },
      "source": [
        "def train_DA(train_loader, val_loader, args,\n",
        "             C, T, D, C_opt, T_opt, D_opt,\n",
        "             crit=nn.CrossEntropyLoss(), D_crit=nn.BCEWithLogitsLoss(),\n",
        "             metric=roc_auc_score):\n",
        "    \n",
        "    def plot_results(epoch):\n",
        "        clear_output(True)\n",
        "        print(\"EPOCH {}\".format(epoch))\n",
        "        \n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(121)\n",
        "        plt.plot(train_stats[\"mean_train_loss\"], label=\"train_loss\")\n",
        "        plt.plot(train_stats[\"mean_val_loss\"], label=\"val_loss\")\n",
        "        plt.plot(train_stats[\"mean_total_loss\"], label=\"total_loss\")\n",
        "        plt.plot(train_stats[\"mean_D_loss\"], label=\"D_loss\")\n",
        "        plt.title(\"losses\")\n",
        "        plt.legend()\n",
        "        plt.subplot(122)\n",
        "        plt.plot(train_stats[\"mean_train_metric\"], label=\"train_metric\")\n",
        "        plt.plot(train_stats[\"mean_val_metric\"], label=\"val_metric\")\n",
        "        plt.gca().set_ylim([0, 1])\n",
        "        plt.title(\"metricss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"  training D_loss (in-iteration): \\t{:.6f}\".format(train_stats[\"mean_D_loss\"][-1]))\n",
        "        print(\"  training total_loss (in-iteration): \\t{:.6f}\".format(train_stats[\"mean_total_loss\"][-1]))\n",
        "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_stats[\"mean_train_loss\"][-1]))\n",
        "        print(\"  validation loss: \\t\\t\\t{:.6f}\".format(train_stats[\"mean_val_loss\"][-1]))\n",
        "        print()\n",
        "        print(\"  training AUC: \\t\\t\\t{:.2f}\".format(train_stats[\"mean_train_metric\"][-1]))\n",
        "        print(\"  validation AUC: \\t\\t\\t{:.2f}\".format(train_stats[\"mean_val_metric\"][-1]))\n",
        "    \n",
        "    train_stats = {\n",
        "        \"mean_D_loss\" : [],\n",
        "        \"mean_total_loss\" : [],\n",
        "        \"mean_train_loss\" : [],\n",
        "        \"mean_val_loss\" : [],\n",
        "        \"mean_train_metric\" : [],\n",
        "        \"mean_val_metric\" : [],\n",
        "    }\n",
        "    \n",
        "    for epoch in range(args[\"n_epochs\"]):\n",
        "        print(\"TRAIN EPOCH {}...\".format(epoch))\n",
        "        train_D_loss = []\n",
        "        train_loss, train_total_loss, train_preds, train_targets = [], [], [], []\n",
        "        val_loss, val_preds, val_targets = [], [], []\n",
        "        \n",
        "        D_n_upd = args[\"D_n_upd\"] if epoch < args[\"D_loop_epochs\"] else 1\n",
        "        if epoch <= args[\"n_pretr_epochs\"]:\n",
        "            cur_lambda = args[\"min_lambda\"]\n",
        "        elif cur_lambda < args[\"max_lambda\"]:\n",
        "            cur_lambda += args[\"inc_lambda\"]\n",
        "        else: \n",
        "            cur_lambda = args[\"max_lambda\"]\n",
        "        print(f\"Current lambda: {cur_lambda}\")\n",
        "        \n",
        "        C.train(True), T.train(True), D.train(True)\n",
        "        for inputs_batch, targets_batch, domains_batch in tqdm(train_loader):\n",
        "            domains_batch = torch.zeros((len(domains_batch), args[\"n_domains\"])).scatter(1, domains_batch.view(-1, 1), 1)\n",
        "            inputs_batch, targets_batch, domains_batch = inputs_batch.float().to(device), targets_batch.long().to(device), domains_batch.float().to(device)\n",
        "\n",
        "            # train D\n",
        "            latents_batch = C(inputs_batch)\n",
        "            for _ in range(D_n_upd):\n",
        "                D_logits_batch = D(latents_batch.detach())\n",
        "                D_loss = D_crit(D_logits_batch, domains_batch)\n",
        "                D_opt.zero_grad()\n",
        "                D_loss.backward()\n",
        "                D_opt.step()\n",
        "            train_D_loss.append(D_loss.item())\n",
        "            \n",
        "            # train C & T\n",
        "            latents_batch = C(inputs_batch)\n",
        "            logits_batch = T(latents_batch)\n",
        "            loss = crit(logits_batch, targets_batch)\n",
        "            D_logits_batch = D(latents_batch)\n",
        "            D_loss = D_crit(D_logits_batch, 1 - domains_batch)\n",
        "            total_loss = loss + cur_lambda * D_loss\n",
        "            total_loss.backward()\n",
        "            T_opt.step(), C_opt.step()\n",
        "            T_opt.zero_grad(), C_opt.zero_grad()\n",
        "            \n",
        "            train_loss.append(loss.item())\n",
        "            train_total_loss.append(total_loss.item())\n",
        "            preds = compute_probs(logits_batch)\n",
        "            train_preds.extend(list(preds.cpu().data.numpy()))\n",
        "            train_targets.extend(list(targets_batch.cpu().data.numpy()))\n",
        "        train_metric = metric(train_targets, train_preds)\n",
        "        \n",
        "        C.train(False), T.train(False)\n",
        "        for inputs_batch, targets_batch, domains_batch in tqdm(val_loader):\n",
        "            inputs_batch, targets_batch, domains_batch = inputs_batch.float().to(device), targets_batch.long().to(device), domains_batch.float().to(device)\n",
        "            \n",
        "            latents_batch = C(inputs_batch)\n",
        "            logits_batch = T(latents_batch)\n",
        "            loss = crit(logits_batch, targets_batch)\n",
        "            val_loss.append(loss.item())\n",
        "            preds = compute_probs(logits_batch)\n",
        "            val_preds.extend(list(preds.cpu().data.numpy()))\n",
        "            val_targets.extend(list(targets_batch.cpu().data.numpy())) \n",
        "        val_metric = metric(val_targets, val_preds)\n",
        "\n",
        "        # save stats\n",
        "        train_stats[\"mean_D_loss\"].append(np.mean(train_D_loss))\n",
        "        train_stats[\"mean_total_loss\"].append(np.mean(train_total_loss))\n",
        "        train_stats[\"mean_train_loss\"].append(np.mean(train_loss))\n",
        "        train_stats[\"mean_val_loss\"].append(np.mean(val_loss))\n",
        "        train_stats[\"mean_train_metric\"].append(train_metric)\n",
        "        train_stats[\"mean_val_metric\"].append(val_metric)\n",
        "        \n",
        "        plot_results(epoch)        \n",
        "    return train_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr-ojrLUIFgu"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4isg3P6fd57x"
      },
      "source": [
        "dataset.use_sources = [\"USM\", \"UCLA\"]\n",
        "\n",
        "# dataset have target classes 1 and 2\n",
        "# encode target with LabelEncoder to use classes 0 and 1 instead\n",
        "# also set the domain target column (\"SOURCE\")\n",
        "dataset.set_target(\"DX_GROUP\", encode_target=True, domain_target=\"SOURCE\")\n",
        "\n",
        "notnull_idx = dataset.target[dataset.target.notnull()].index\n",
        "train_idx, val_idx = train_test_split(notnull_idx, stratify=dataset.target[notnull_idx],\n",
        "                                      test_size=0.2, random_state=42)\n",
        "batch_size = 16\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    data.Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    data.Subset(dataset, val_idx), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "C_model = ConvEncoder(input_shape=(48, 64, 48), \n",
        "                      conv_model=[32, 64, 128, 256], \n",
        "                      conv_model_type=\"CNN\",\n",
        "                      n_flatten_units=None).to(device)\n",
        "T_model = ClfGRU(n_latent_units=C_model.conv_model.n_flatten_units, \n",
        "                 seq_length=16, \n",
        "                 n_outputs=2,\n",
        "                 hidden_size=64,\n",
        "                 n_layers=1,\n",
        "                 use_states=\"mean\",\n",
        "                 n_fc_units=128,\n",
        "                 dropout=0.2,).to(device)\n",
        "D_model = DiscModel(n_latent_units=C_model.conv_model.n_flatten_units, \n",
        "                    n_domains=len(dataset.use_sources),\n",
        "                    fc_model=[1024, 256, 64],\n",
        "                    batchnorm=True,\n",
        "                    dropout=0.3).to(device)\n",
        "lr = 1e-4\n",
        "wd = 0           \n",
        "C_opt = torch.optim.Adam(C_model.parameters(), lr=lr, weight_decay=wd)\n",
        "T_opt = torch.optim.Adam(T_model.parameters(), lr=lr, weight_decay=wd)\n",
        "D_opt = torch.optim.Adam(D_model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "args = {\n",
        "    \"n_domains\" : len(dataset.use_sources),\n",
        "    \"n_epochs\" : 25,\n",
        "    \"n_pretr_epochs\" : 0,\n",
        "    \"min_lambda\" : 0,\n",
        "    \"inc_lambda\" : 3e-4 / 20,\n",
        "    \"max_lambda\" : 3e-4,\n",
        "    \"D_loop_epochs\" : 15,\n",
        "    \"D_n_upd\" : 3,\n",
        "}\n",
        "\n",
        "train_stats = train_DA(train_loader, val_loader, args,\n",
        "                       C_model, T_model, D_model, C_opt, T_opt, D_opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6qg3V4SXmc7"
      },
      "source": [
        "latents = []\n",
        "dataset.set_target()\n",
        "C_model.train(False)\n",
        "for i in tqdm(notnull_idx):\n",
        "    x = dataset[i][[0]].unsqueeze(dim=0).to(device)\n",
        "    z = C_model(x)[0].cpu().detach().numpy()\n",
        "    latents.append(z)\n",
        "latents = np.concatenate(latents, axis=0)\n",
        "\n",
        "pca = PCA(n_components=50)\n",
        "tsne = TSNE(n_components=2)\n",
        "latents_2d = tsne.fit_transform(pca.fit_transform(latents))\n",
        "\n",
        "source_colors = {\"USM\" : \"tab:blue\",\n",
        "          \"UCLA\" : \"tab:orange\",}\n",
        "sources = np.array(dataset.labels.loc[notnull_idx, \"SOURCE\"].tolist())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for s in source_colors:\n",
        "    s_idx = (sources == s)\n",
        "    plt.scatter(latents_2d[s_idx, 0], latents_2d[s_idx, 1], \n",
        "                s=30, alpha=0.75, c=source_colors[s], label=s)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRVpzo1J-AYc"
      },
      "source": [
        "dataset.use_sources = [\"USM\", \"UCLA\"]\n",
        "\n",
        "# dataset have target classes 1 and 2\n",
        "# encode target with LabelEncoder to use classes 0 and 1 instead\n",
        "# also set the domain target column (\"SOURCE\")\n",
        "dataset.set_target(\"DX_GROUP\", encode_target=True, domain_target=\"SOURCE\")\n",
        "\n",
        "notnull_idx = dataset.target[dataset.target.notnull()].index\n",
        "train_idx, val_idx = train_test_split(notnull_idx, stratify=dataset.target[notnull_idx],\n",
        "                                      test_size=0.2, random_state=42)\n",
        "batch_size = 16\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    data.Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    data.Subset(dataset, val_idx), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "C_model = ConvEncoder(input_shape=(48, 64, 48), \n",
        "                      conv_model=[32, 64, 128, 256], \n",
        "                      conv_model_type=\"CNN\",\n",
        "                      n_flatten_units=None).to(device)\n",
        "T_model = ClfGRU(n_latent_units=C_model.conv_model.n_flatten_units, \n",
        "                 seq_length=16, \n",
        "                 n_outputs=2,\n",
        "                 hidden_size=64,\n",
        "                 n_layers=1,\n",
        "                 use_states=\"mean\",\n",
        "                 n_fc_units=128,\n",
        "                 dropout=0.2,).to(device)\n",
        "D_model = DiscModel(n_latent_units=C_model.conv_model.n_flatten_units, \n",
        "                    n_domains=len(dataset.use_sources),\n",
        "                    fc_model=[1024, 256, 64],\n",
        "                    batchnorm=True,\n",
        "                    dropout=0.3).to(device)\n",
        "lr = 1e-4\n",
        "wd = 0           \n",
        "C_opt = torch.optim.Adam(C_model.parameters(), lr=lr, weight_decay=wd)\n",
        "T_opt = torch.optim.Adam(T_model.parameters(), lr=lr, weight_decay=wd)\n",
        "D_opt = torch.optim.Adam(D_model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "args = {\n",
        "    \"n_domains\" : len(dataset.use_sources),\n",
        "    \"n_epochs\" : 25,\n",
        "    \"n_pretr_epochs\" : 0,\n",
        "    \"min_lambda\" : 0,\n",
        "    \"inc_lambda\" : 3e-3 / 20,\n",
        "    \"max_lambda\" : 3e-3,\n",
        "    \"D_loop_epochs\" : 15,\n",
        "    \"D_n_upd\" : 3,\n",
        "}\n",
        "\n",
        "train_stats = train_DA(train_loader, val_loader, args,\n",
        "                       C_model, T_model, D_model, C_opt, T_opt, D_opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krOVemKzXmfj"
      },
      "source": [
        "latents = []\n",
        "dataset.set_target()\n",
        "C_model.train(False)\n",
        "for i in tqdm(notnull_idx):\n",
        "    x = dataset[i][[0]].unsqueeze(dim=0).to(device)\n",
        "    z = C_model(x)[0].cpu().detach().numpy()\n",
        "    latents.append(z)\n",
        "latents = np.concatenate(latents, axis=0)\n",
        "\n",
        "pca = PCA(n_components=50)\n",
        "tsne = TSNE(n_components=2)\n",
        "latents_2d = tsne.fit_transform(pca.fit_transform(latents))\n",
        "\n",
        "source_colors = {\"USM\" : \"tab:blue\",\n",
        "          \"UCLA\" : \"tab:orange\",}\n",
        "sources = np.array(dataset.labels.loc[notnull_idx, \"SOURCE\"].tolist())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for s in source_colors:\n",
        "    s_idx = (sources == s)\n",
        "    plt.scatter(latents_2d[s_idx, 0], latents_2d[s_idx, 1], \n",
        "                s=30, alpha=0.75, c=source_colors[s], label=s)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUV2nggOaXHt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}