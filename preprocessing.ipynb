{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# fMRI data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adasegroup/NEUROML2020/blob/seminar4/seminar-4/preprocessing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nibabel\n",
    "from nilearn import plotting\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "anat = nibabel.load('/data/ds000114/derivatives/fmriprep/sub-01/anat/sub-01_t1w_preproc.nii.gz')\n",
    "fmri = nibabel.load('/data/ds000114/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_bold.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "In this workflow we will conduct the following steps:\n",
    "\n",
    "**1. Coregistration of functional images to anatomical images (according to FSL's FEAT pipeline)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Co-registrationis the process of spatial alignment of 2 images. The target image is also called reference volume. The goodness of alignment is evaluated with a cost function.\n",
    "\n",
    "We have to move the fmri series from fmri native space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plotting.view_img(nibabel.nifti1.Nifti1Image(fmri.get_data()[:,:,:,1], affine=fmri.affine), bg_img=anat, threshold=0.1e3, cut_coords=(0,0,0), title='Anat and fmri misalignment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " To native anatomical space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2. Motion correction of functional images with FSL's MCFLIRT**\n",
    "\n",
    " The images are aligned with rigid transformation - rotations, translations, reflections.\n",
    " Then spatial interpolation is done, so as there was no movements.\n",
    "\n",
    "![Rigit transformation](https://www.researchgate.net/profile/Olivier_Serres/publication/43808029/figure/fig4/AS:304436623757316@1449594755197/Rigid-body-transformation-scale-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " **3. Slice Timing correction**\n",
    " \n",
    " The brain slices are not acquired at the same time. Therefore, interpolation is done between the nearest timepoints\n",
    "![Slice Order](https://crnl.readthedocs.io/_images/slice_order_1.jpg)\n",
    "\n",
    "[Slice timing corretion in python](https://matthew-brett.github.io/teaching/slice_timing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**4. Smoothing of coregistered functional images with FWHM set to 5/10 mm**\n",
    "\n",
    "**5. Artifact Detection in functional images (to detect outlier volumes)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**So, let's start!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imports\n",
    "\n",
    "First, let's import all the modules we later will be needing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "%matplotlib inline\n",
    "from os.path import join as opj\n",
    "import os\n",
    "import json\n",
    "from nipype.interfaces.fsl import (BET, ExtractROI, FAST, FLIRT, ImageMaths,\n",
    "                                   MCFLIRT, SliceTimer, Threshold)\n",
    "from nipype.interfaces.spm import Smooth\n",
    "from nipype.interfaces.utility import IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.algorithms.rapidart import ArtifactDetect\n",
    "from nipype import Workflow, Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiment parameters\n",
    "\n",
    "It's always a good idea to specify all parameters that might change between experiments at the beginning of your script. We will use one functional image for fingerfootlips task for ten subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "experiment_dir = '/output'\n",
    "#output_dir = '/home/neuro/nipype_tutorial/notebooks'\n",
    "output_dir = 'datasink'\n",
    "working_dir = 'workingdir'\n",
    "\n",
    "# list of subject identifiers\n",
    "#subject_list = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10']\n",
    "\n",
    "subject_list = ['03', '04']\n",
    "# list of session identifiers\n",
    "task_list = ['fingerfootlips']\n",
    "\n",
    "# Smoothing widths to apply\n",
    "fwhm = [5, 10]\n",
    "\n",
    "# TR of functional images(time from the application of an excitation pulse to the application of the next pulse)\n",
    "\n",
    "with open('/data/ds000114/task-fingerfootlips_bold.json', 'rt') as fp:\n",
    "    task_info = json.load(fp)\n",
    "TR = task_info['RepetitionTime']\n",
    "\n",
    "# Isometric resample of functional images to voxel size (in mm)\n",
    "iso_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Specify Nodes for the main workflow\n",
    "\n",
    "Initiate all the different interfaces (represented as nodes) that you want to use in your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# ExtractROI - skip dummy scans\n",
    "#t_min - Minimum index for t-dimension\n",
    "#t_size - Size of ROI in t-dimension\n",
    "extract = Node(ExtractROI(t_min=4, t_size=-1, output_type='NIFTI'),\n",
    "               name=\"extract\")\n",
    "\n",
    "#MCFLIRT - motion correction\n",
    "#https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MCFLIRT\n",
    "#mean_vol- volumes are averaged to create a new template\n",
    "#normcorr cost - https://www.fmrib.ox.ac.uk/datasets/techrep/tr02mj1/tr02mj1/node4.html\n",
    "#https://onlinelibrary.wiley.com/reader/content/15ee5e5c909/10.1002/hbm.20235/format/pdf/OEBPS/pages/bg2.png\n",
    "#sinc interpolation - https://math.stackexchange.com/questions/1372632/how-does-sinc-interpolation-work\n",
    "mcflirt = Node(MCFLIRT(mean_vol=True,\n",
    "                       save_plots=True,\n",
    "                       output_type='NIFTI'),\n",
    "               name=\"mcflirt\")\n",
    "\n",
    "#SliceTimer - correct for slice wise acquisition\n",
    "#https://poc.vl-e.nl/distribution/manual/fsl-3.2/slicetimer/index.html\n",
    "#more on https://matthew-brett.github.io/teaching/slice_timing.html\n",
    "#interleaved = -odd\n",
    "#top to bottom = --down\n",
    "#normcorr loss\n",
    "slicetimer = Node(SliceTimer(index_dir=False,\n",
    "                             interleaved=True,\n",
    "                             output_type='NIFTI',\n",
    "                             time_repetition=TR),\n",
    "                  name=\"slicetimer\")\n",
    "\n",
    "#Smooth - image smoothing\n",
    "#spm_smooth for 3D Gaussian smoothing\n",
    "smooth = Node(Smooth(), name=\"smooth\")\n",
    "smooth.iterables = (\"fwhm\", fwhm)\n",
    "\n",
    "# Artifact Detection - determines outliers in functional images via intensity and motion paramters\n",
    "#http://web.mit.edu/swg/art/art.pdf\n",
    "#norm_threshold - Threshold to use to detect motion-related outliers when composite motion is being used\n",
    "#zintensity_threshold - Intensity Z-threshold use to detection images that deviate from the mean\n",
    "#spm_global like calculation to determine the brain mask\n",
    "#parameter_source - Source of movement parameters\n",
    "#use_differences - Use differences between successive motion (first element) and\n",
    "#intensity parameter (second element) estimates in order to determine outliers.\n",
    "\n",
    "art = Node(ArtifactDetect(norm_threshold=2,\n",
    "                          zintensity_threshold=3,\n",
    "                          mask_type='spm_global',\n",
    "                          parameter_source='FSL',\n",
    "                          use_differences=[True, False],\n",
    "                          plot_type='svg'),\n",
    "           name=\"art\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coregistration Workflow\n",
    "\n",
    "Initiate a workflow that coregistrates the functional images to the anatomical image (according to FSL's FEAT pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# BET - Skullstrip anatomical Image\n",
    "#https://www.fmrib.ox.ac.uk/datasets/techrep/tr00ss2/tr00ss2.pdf\n",
    "\n",
    "bet_anat = Node(BET(frac=0.5,\n",
    "                    robust=True,\n",
    "                    output_type='NIFTI_GZ'),\n",
    "                name=\"bet_anat\")\n",
    "\n",
    "# FAST - Image Segmentation\n",
    "#https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FAST\n",
    "#http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.200.3832&rep=rep1&type=pdf\n",
    "\n",
    "segmentation = Node(FAST(output_type='NIFTI_GZ'),\n",
    "                    name=\"segmentation\", mem_gb=4)\n",
    "\n",
    "# Select WM segmentation file from segmentation output\n",
    "# Get better boundaries on white and gray matter\n",
    "def get_wm(files):\n",
    "    return files[-1]\n",
    "\n",
    "# Threshold - Threshold WM probability image\n",
    "threshold = Node(Threshold(thresh=0.5,\n",
    "                           args='-bin',\n",
    "                           output_type='NIFTI_GZ'),\n",
    "                name=\"threshold\")\n",
    "\n",
    "# FLIRT - pre-alignment of functional images to anatomical images\n",
    "#https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT\n",
    "coreg_pre = Node(FLIRT(dof=6, output_type='NIFTI_GZ'),\n",
    "                 name=\"coreg_pre\")\n",
    "\n",
    "# FLIRT - coregistration of functional images to anatomical images with BBR(uses the segmentation)\n",
    "#https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT_BBR\n",
    "coreg_bbr = Node(FLIRT(dof=6,\n",
    "                       cost='bbr',\n",
    "                       schedule=opj(os.getenv('FSLDIR'),\n",
    "                                    'etc/flirtsch/bbr.sch'),\n",
    "                       output_type='NIFTI_GZ'),\n",
    "                 name=\"coreg_bbr\")\n",
    "\n",
    "# Apply coregistration warp to functional images\n",
    "#apply_isoxfm-apply transformation supplied by in_matrix_file\n",
    "applywarp = Node(FLIRT(interp='spline',\n",
    "                       apply_isoxfm=iso_size,\n",
    "                       output_type='NIFTI'),\n",
    "                 name=\"applywarp\")\n",
    "\n",
    "# Apply coregistration wrap to mean file\n",
    "applywarp_mean = Node(FLIRT(interp='spline',\n",
    "                            apply_isoxfm=iso_size,\n",
    "                            output_type='NIFTI_GZ'),\n",
    "                 name=\"applywarp_mean\")\n",
    "\n",
    "# Create a coregistration workflow\n",
    "coregwf = Workflow(name='coregwf')\n",
    "coregwf.base_dir = opj(experiment_dir, working_dir)\n",
    "\n",
    "# Connect all components of the coregistration workflow\n",
    "coregwf.connect([(bet_anat, segmentation, [('out_file', 'in_files')]),\n",
    "                 (segmentation, threshold, [(('partial_volume_files', get_wm),\n",
    "                                             'in_file')]),\n",
    "                 (bet_anat, coreg_pre, [('out_file', 'reference')]),\n",
    "                 (threshold, coreg_bbr, [('out_file', 'wm_seg')]),\n",
    "                 (coreg_pre, coreg_bbr, [('out_matrix_file', 'in_matrix_file')]),\n",
    "                 (coreg_bbr, applywarp, [('out_matrix_file', 'in_matrix_file')]),\n",
    "                 (bet_anat, applywarp, [('out_file', 'reference')]),\n",
    "                 (coreg_bbr, applywarp_mean, [('out_matrix_file', 'in_matrix_file')]),\n",
    "                 (bet_anat, applywarp_mean, [('out_file', 'reference')]),\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Specify input & output stream\n",
    "\n",
    "Specify where the input data can be found & where and how to save the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Infosource - a function free node to iterate over the list of subject names\n",
    "infosource = Node(IdentityInterface(fields=['subject_id', 'task_name']),\n",
    "                  name=\"infosource\")\n",
    "infosource.iterables = [('subject_id', subject_list),\n",
    "                        ('task_name', task_list)]\n",
    "\n",
    "# SelectFiles - to grab the data \n",
    "anat_file = opj('derivatives', 'fmriprep', 'sub-{subject_id}', 'anat', 'sub-{subject_id}_t1w_preproc.nii.gz')\n",
    "func_file = opj('sub-{subject_id}', 'ses-test', 'func',\n",
    "                'sub-{subject_id}_ses-test_task-{task_name}_bold.nii.gz')\n",
    "\n",
    "templates = {'anat': anat_file,\n",
    "             'func': func_file}\n",
    "selectfiles = Node(SelectFiles(templates,\n",
    "                               base_directory='/data/ds000114'),\n",
    "                   name=\"selectfiles\")\n",
    "\n",
    "# Datasink - creates output folder for important outputs\n",
    "datasink = Node(DataSink(base_directory=experiment_dir,\n",
    "                         container=output_dir),\n",
    "                name=\"datasink\")\n",
    "\n",
    "## Use the following DataSink output substitutions\n",
    "substitutions = [('_subject_id_', 'sub-'),\n",
    "                 ('_task_name_', '/task-'),\n",
    "                 ('_fwhm_', 'fwhm-'),\n",
    "                 ('_roi', ''),\n",
    "                 ('_mcf', ''),\n",
    "                 ('_st', ''),\n",
    "                 ('_flirt', ''),\n",
    "                 ('.nii_mean_reg', '_mean'),\n",
    "                 ('.nii.par', '.par'),\n",
    "                 ]\n",
    "subjFolders = [('fwhm-%s/' % f, 'fwhm-%s_' % f) for f in fwhm]\n",
    "substitutions.extend(subjFolders)\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Specify Workflow\n",
    "\n",
    "Create a workflow and connect the interface nodes and the I/O stream to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a preprocessing workflow\n",
    "preproc = Workflow(name='preproc')\n",
    "preproc.base_dir = opj(experiment_dir, working_dir)\n",
    "\n",
    "# Connect all components of the preprocessing workflow\n",
    "preproc.connect([(infosource, selectfiles, [('subject_id', 'subject_id'),\n",
    "                                            ('task_name', 'task_name')]),\n",
    "                 (selectfiles, extract, [('func', 'in_file')]),\n",
    "                 (extract, mcflirt, [('roi_file', 'in_file')]),\n",
    "                 (mcflirt, slicetimer, [('out_file', 'in_file')]),\n",
    "\n",
    "                 (selectfiles, coregwf, [('anat', 'bet_anat.in_file'),\n",
    "                                         ('anat', 'coreg_bbr.reference')]),\n",
    "                 (mcflirt, coregwf, [('mean_img', 'coreg_pre.in_file'),\n",
    "                                     ('mean_img', 'coreg_bbr.in_file'),\n",
    "                                     ('mean_img', 'applywarp_mean.in_file')]),\n",
    "                 (slicetimer, coregwf, [('slice_time_corrected_file', 'applywarp.in_file')]),\n",
    "                 \n",
    "                 (coregwf, smooth, [('applywarp.out_file', 'in_files')]),\n",
    "\n",
    "                 (mcflirt, datasink, [('par_file', 'preproc.@par')]),\n",
    "                 (smooth, datasink, [('smoothed_files', 'preproc.@smooth')]),\n",
    "                 (coregwf, datasink, [('applywarp_mean.out_file', 'preproc.@mean')]),\n",
    "\n",
    "                 (coregwf, art, [('applywarp.out_file', 'realigned_files')]),\n",
    "                 (mcflirt, art, [('par_file', 'realignment_parameters')]),\n",
    "\n",
    "                 (coregwf, datasink, [('coreg_bbr.out_matrix_file', 'preproc.@mat_file'),\n",
    "                                      ('bet_anat.out_file', 'preproc.@brain')]),\n",
    "                 (art, datasink, [('outlier_files', 'preproc.@outlier_files'),\n",
    "                                  ('plot_files', 'preproc.@plot_files')]),\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualize the workflow\n",
    "\n",
    "It always helps to visualize your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create preproc output graph\n",
    "preproc.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "\n",
    "# Visualize the graph\n",
    "from IPython.display import Image\n",
    "Image(filename=opj(preproc.base_dir, 'preproc', 'graph.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the detailed graph\n",
    "preproc.write_graph(graph2use='flat', format='png', simple_form=True)\n",
    "Image(filename=opj(preproc.base_dir, 'preproc', 'graph_detailed.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Run the Workflow\n",
    "\n",
    "Now that everything is ready, we can run the preprocessing workflow. Change ``n_procs`` to the number of jobs/cores you want to use. **Note** that if  you're using a Docker container and FLIRT fails to run without any good reason, you might need to change memory settings in the Docker preferences (6 GB should be enough for this workflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "args_dict = {'n_procs' : 4}\n",
    "preproc.run('MultiProc', plugin_args=args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Inspect output\n",
    "\n",
    "Let's check the structure of the output folder, to see if we have everything we wanted to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!tree /output/datasink/preproc/sub-01/task-fingerfootlips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualize results\n",
    "\n",
    "Let's check the effect of the different smoothing kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nilearn\n",
    "from nilearn import image, plotting\n",
    "out_path = '/output/datasink/preproc/sub-01/task-fingerfootlips/'\n",
    "fmri_preproc = nilearn.image.load_img(f'{out_path}/sub-01_ses-test_task-fingerfootlips_bold_mean.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plotting.view_img(fmri_preproc, bg_img=anat, threshold=0.1e3, cut_coords=(0,0,0), title='Anat and fmri aligned, fwhm = 0 mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fmri_preproc_5 = image.mean_img(nilearn.image.load_img(f'{out_path}/fwhm-5_ssub-01_ses-test_task-fingerfootlips_bold.nii'))\n",
    "plotting.view_img(fmri_preproc_5, bg_img=anat, threshold=0.1e3, cut_coords=(0,0,0), title='Anat and fmri aligned, fwhm = 5 mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_preproc_10 = image.mean_img(nilearn.image.load_img(f'{out_path}/fwhm-10_ssub-01_ses-test_task-fingerfootlips_bold.nii'))\n",
    "plotting.view_img(fmri_preproc_10, bg_img=anat, threshold=0.1e3, cut_coords=(0,0,0), title='Anat and fmri aligned, fwhm = 5 mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's investigate the motion parameters. How much did the subject move and turn in the scanner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "par = np.loadtxt('/output/datasink/preproc/sub-01/task-fingerfootlips/sub-01_ses-test_task-fingerfootlips_bold.par')\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 5))\n",
    "axes[0].set_ylabel('rotation (radians)')\n",
    "axes[0].plot(par[0:, :3])\n",
    "axes[1].plot(par[0:, 3:])\n",
    "axes[1].set_xlabel('time (TR)')\n",
    "axes[1].set_ylabel('translation (mm)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There seems to be a rather drastic motion around volume 102. Let's check if the outliers detection algorithm was able to pick this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "outlier_ids = np.loadtxt('/output/datasink/preproc/sub-01/task-fingerfootlips/art.sub-01_ses-test_task-fingerfootlips_bold_outliers.txt')\n",
    "print('Outliers were detected at volumes: %s' % outlier_ids)\n",
    "\n",
    "from IPython.display import SVG\n",
    "SVG(filename='/output/datasink/preproc/sub-01/task-fingerfootlips/plot.sub-01_ses-test_task-fingerfootlips_bold.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternative for motion artifacts detection [ICA-based Automatic Removal Of Motion Artifact](https://nipype.readthedocs.io/en/0.13.1/interfaces/generated/interfaces.fsl/ICA_AROMA.html)\n",
    "\n",
    "\n",
    "**Dataset:**\n",
    "[A test-retest fMRI dataset for motor, language and spatial attention functions](https://openneuro.org/datasets/ds000114/versions/1.0.1)\n",
    "\n",
    "**Special thanks to Michael Notter for the wonderful [nipype tutorial](https://miykael.github.io/nipype_tutorial/)**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
